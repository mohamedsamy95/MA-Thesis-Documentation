\chapter{Experiments}
\label{chap:experiments}
In this chapter, we describe a set of experiments we ran to quantitatively and qualitatively measure the effect of incorporating the log embeddings introduced into normal behavior models
when applied on both \emph{healthy} and \emph{faulty} turbines. For a normal behavior model monitoring a component of a turbine, we considered this turbine \emph{faulty} if 
a failure was reported, in the failures dataset, related to this specific component of this turbine. It is considered \emph{healthy} if no failures, relating to this turbine's component,
were reported.\\
To compare different models in an identical setup, we use the following metrics:
\begin{bulletList}
    \item \textbf{Root Mean Squared Error ($RMSE$)}: It is a commonly used metric to evaluate the performance of a predictive model or an estimator.
    The $RMSE$ is calculated as the square root of the mean of the squared differences between the predicted ($y_{predicted}$) and actual values ($y_{actual}$), or as follows:
    \begin{equation}
        RMSE = \sqrt{\frac{1}{n} * \sum_{i}^{n} (y_{predicted}^i - y_{actual}^i)^2}
    \end{equation}
    where $n$ is the number of data points in a dataset. The RMSE is expressed in the same units as the original data. 
    As a rule of thumb: The lower the RMSE, the better the model fits the data.
    \item \textbf{Numbers of anomalies and alarms detected during a given period}: We use these numbers to measure the capability of a model to detect/predict a failure. 
    The number of anomalies detected reflects the total number of anomalous data points, whereas the number of alarms detected counts only the number of operation days of a turbine 
    where the system notified the operator of a potential failure by sending an alarm (if the number of anomalies detected in a day exceeds a certain threshold, 
    as explained in \ref{subsub:anvsal}). When compared to another model, we consider a model more \emph{capable} of predicting failures if it 
    detects more anomalies and/or sends more alarms during the time of abnormal operation of a faulty turbine given that it reported no anomalies or alarms during the normal operation 
    of the same turbine. In other words, we compare these metrics between models when applied to the test data of a faulty turbine, assuming that the data used to train these models 
    was collected from the turbine in a period when it was operating in a healthy state; hence no anomalies should be detected in this period.
    \item \textbf{Timestamps of the first anomaly detected and alarm sent}: Used to compare the capability of different models to early-detect failures, when applied on a faulty turbine. The earlier
    the first anomaly is detected or the first alarm is sent the better.
\end{bulletList}
All the condition monitoring normal behavior models used in our experiments were trained to monitor the generator bearings of a turbine (i.e., having the average temperature in the generator bearings 
as a target), whereas the power curve normal behavior models monitor the average power production of a turbine according to the grid (in kW). The input features used are listed in \ref{sub:featselect}.

%Experiment I
\section{Benchmark NBM architecture}
\label{exp:I}

In the early stages of this work, we trained linear regression models due to their lightweight and low computational power needed. Knowing that they are incapable of capturing non-linear
relationships in the data, we assumed that the linear regression models would be outperformed by feed-forward neural networks when it comes down to fitting the 
signals data of a healthy turbine. To test this hypothesis and select a specific architecture to be used as a benchmark NBM model in other experiments, we did this simple experiment 
to compare the $RMSE$ scores of both models.\\
This experiment was conducted on a healthy turbine (Turbine 01). Both models were trained on signals data collected between 01/09/2016 and 30/08/2017 and tested on data collected between
01/09/2017 and 31/12/2017.\\
As shown in Table \ref{tab:Experiment I results}, the feed-forward network outperformed the linear regression model---as expected---and was used as a baseline in all the other experiments.
\begin{table}[H]
        \centering
    \begin{tabular}{|c|c|c|}
    \hline
         \textbf{Metric} & \textbf{Linear regression} & \textbf{Feed-forward network}\\
         \hline
         Training RMSE & 5.29 & 4.86\\
         \hline
         Testing RMSE & 5.80 & 5.78 \\
    \hline
    \end{tabular}
    \caption{Experiment I results: RMSEs measured and used to compare between the benchmark models}
        \label{tab:Experiment I results}
\end{table}

%Experiment II

\section{Effect of incorporating log embeddings into NBM for condition monitoring when applied on a healthy turbine (T01)}

\subsection{Research question}
The aim of this experiment is to quantitatively (RMSE) and qualitatively (number of false alarms) measure the effect of incorporating SCADA-log-based features into the benchmark NBM.

\subsection{Setup}
The following elements were used in this experiment:
\begin{bulletList}
    \item \textbf{Machine learning models:} Feed-forward neural network with single target features and Feed-forward neural network with multiple target features
    \item \textbf{Target wind turbine:} T01
    \item \textbf{Dataset:} Training/healthy period: 01/09/2016 - 31/12/2016, Testing period: 01/01/2017 - 31/12/2017
    \item \textbf{Input features (SCADA signals):} Nac\_Temp\_Avg, Amb\_Temp\_Avg, Gen\_RPM\_Avg, Prod\_LatestAvg\_TotActPwr (or use verbose names of signals: the average temperature in the nacelle, average ambient temperature, average generator rpm, total active power)
    \item \textbf{SCADA-log-based input features:} Operation and System log messages containing the word "vent", which resulted in four different features extracted from the following components: Generator external vent, Generator internal vent, High-voltage transformer vent, and Nacelle vent
    \item \textbf{Target feature for single-output model: } Gen\_Bear\_Temp\_Avg (Average temperature in generator bearing 1 (Non-Drive End))
    \item \textbf{Target features for multiple-output model:} All signals whose names contain the keywords "Gen" and "Temp": 'Gen\_Bear\_Temp\_Avg', 'Gen\_Phase1\_Temp\_Avg', 'Gen\_Phase2\_Temp\_Avg', 'Gen\_Phase3\_Temp\_Avg', 'Gen\_SlipRing\_Temp\_Avg', 'Gen\_Bear2\_Temp\_Avg'
    \item \textbf{Recorded failure:} No generator-related recorded failures (hence the assumption that the turbine is healthy)
\end{bulletList}

\subsection{Results}
According to the results documented in \ref{tab:Experiment I results}, we conclude that both NBMs are capable of predicting the failure in the monitored part. We will, however, use only the feed-forward network model as a benchmark since it outperformed the linear regression model.
\begin{table}[H]
        \centering
    \begin{tabular}{|m{4cm}|m{4cm}|m{4cm}|}
    \hline
         \textbf{Comparison metric} & \textbf{Measure for linear regression}   & \textbf{Measure for feed-forward network}\\
         \hline
         RMSE & & \\
         \hline
         First-detected anomaly timestamp & & \\
         \hline
         Number of anomalies detected & & \\
         \hline
    \hline
    \end{tabular}
    \caption{Experiment I results: Metrics used to compare between the benchmark models}
        \label{tab:Experiment II results}
\end{table}

\section{Effect of incorporating log embeddings into NBM for condition monitoring when applied on a faulty turbine (T09)}

\section{Effect of incorporating log embeddings into NBM for condition monitoring when applied on a faulty turbine (T06)}

\section{Effect of log-based data filtering on NBM for power curve modeling when applied on T01}